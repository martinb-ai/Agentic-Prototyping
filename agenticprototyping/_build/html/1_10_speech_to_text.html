
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Speech to text &#8212; Agentic Prototyping</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '1_10_speech_to_text';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Vector embeddings" href="1_11_embeddings.html" />
    <link rel="prev" title="Text to speech" href="1_9_text_to_speech.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/ITRGLogoLightBlue.png" class="logo__image only-light" alt="Agentic Prototyping - Home"/>
    <script>document.write(`<img src="_static/ITRGLogoLightBlue.png" class="logo__image only-dark" alt="Agentic Prototyping - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to Agentic Prototyping
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Core Concepts</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_0_dev_quick_start.html">Developer Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_1_model_selection.html">Model Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_2_prototype_to_production.html">Prototype to Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_3_structured_outputs.html">Structured Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_4_function_calling.html">Function calling</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_5_conversational_state.html">Conversation state</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_6_streaming.html">Streaming API responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_7_file_inputs.html">File inputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_8_image_generation.html">Image generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_9_text_to_speech.html">Text to speech</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Speech to text</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_11_embeddings.html">Vector embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_12_retrieval.html">Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_13_evals.html">Evaluating model performance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Agents</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2_0_agents_overview.html">Agents Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_1_agent_design_foundations.html">Agent Design Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_2_guardrails.html">Guardrails</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Best Practices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="3_0_production.html">Production best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_1_safety.html">Safety Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_2_prompt_caching.html">Prompt caching</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_3_reasoning_models.html">Reasoning Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_4_accuracy_optimization.html">Optimizing LLM Accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_5_reproducibility.html">Advanced usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_6_evaluation_design.html">Evals design best practices</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Case Studies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="4_pharma_rd.html">Use Case: AI Co-Scientist for Pharma R&amp;D</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_insurance_claims.html">Use Case: Insurance Claim Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_financial_portfolio_analysis.html">Use Case: Financial Portfolio Analysis</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/1_10_speech_to_text.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Speech to text</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quickstart">Quickstart</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transcriptions">Transcriptions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#translations">Translations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-languages">Supported languages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#timestamps">Timestamps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#longer-inputs">Longer inputs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting">Prompting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#streaming-transcriptions">Streaming transcriptions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#streaming-the-transcription-of-a-completed-audio-recording">Streaming the transcription of a completed audio recording</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#streaming-the-transcription-of-an-ongoing-audio-recording">Streaming the transcription of an ongoing audio recording</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-reliability">Improving reliability</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="speech-to-text">
<h1>Speech to text<a class="headerlink" href="#speech-to-text" title="Link to this heading">#</a></h1>
<p>Learn how to turn audio into text.</p>
<p>The Audio API provides two speech to text endpoints:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">transcriptions</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">translations</span></code></p></li>
</ul>
<p>Historically, both endpoints have been backed by our open source <a class="reference external" href="https://openai.com/blog/whisper/">Whisper model</a> (<code class="docutils literal notranslate"><span class="pre">whisper-1</span></code>). The <code class="docutils literal notranslate"><span class="pre">transcriptions</span></code> endpoint now also supports higher quality model snapshots, with limited parameter support:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-4o-mini-transcribe</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-4o-transcribe</span></code></p></li>
</ul>
<p>All endpoints can be used to:</p>
<ul class="simple">
<li><p>Transcribe audio into whatever language the audio is in.</p></li>
<li><p>Translate and transcribe the audio into English.</p></li>
</ul>
<p>File uploads are currently limited to 25 MB, and the following input file types are supported: <code class="docutils literal notranslate"><span class="pre">mp3</span></code>, <code class="docutils literal notranslate"><span class="pre">mp4</span></code>, <code class="docutils literal notranslate"><span class="pre">mpeg</span></code>, <code class="docutils literal notranslate"><span class="pre">mpga</span></code>, <code class="docutils literal notranslate"><span class="pre">m4a</span></code>, <code class="docutils literal notranslate"><span class="pre">wav</span></code>, and <code class="docutils literal notranslate"><span class="pre">webm</span></code>.</p>
<section id="quickstart">
<h2>Quickstart<a class="headerlink" href="#quickstart" title="Link to this heading">#</a></h2>
<section id="transcriptions">
<h3>Transcriptions<a class="headerlink" href="#transcriptions" title="Link to this heading">#</a></h3>
<p>The transcriptions API takes as input the audio file you want to transcribe and the desired output file format for the transcription of the audio. All models support the same set of input formats. On output, <code class="docutils literal notranslate"><span class="pre">whisper-1</span></code> supports a range of formats (<code class="docutils literal notranslate"><span class="pre">json</span></code>, <code class="docutils literal notranslate"><span class="pre">text</span></code>, <code class="docutils literal notranslate"><span class="pre">srt</span></code>, <code class="docutils literal notranslate"><span class="pre">verbose_json</span></code>, <code class="docutils literal notranslate"><span class="pre">vtt</span></code>); the newer <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini-transcribe</span></code> and <code class="docutils literal notranslate"><span class="pre">gpt-4o-transcribe</span></code> snapshots currently only support <code class="docutils literal notranslate"><span class="pre">json</span></code> or plain <code class="docutils literal notranslate"><span class="pre">text</span></code> responses.</p>
<p>Transcribe audio</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
<span class="n">audio_file</span><span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;/path/to/file/audio.mp3&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span>

<span class="n">transcription</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">audio</span><span class="o">.</span><span class="n">transcriptions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-transcribe&quot;</span><span class="p">,</span> 
    <span class="n">file</span><span class="o">=</span><span class="n">audio_file</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">transcription</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<p>By default, the response type will be json with the raw text included.</p>
<p>{ “text”: “Imagine the wildest idea that you’ve ever had, and you’re curious about how it might scale to something that’s a 100, a 1,000 times bigger. …. }</p>
<p>The Audio API also allows you to set additional parameters in a request. For example, if you want to set the <code class="docutils literal notranslate"><span class="pre">response_format</span></code> as <code class="docutils literal notranslate"><span class="pre">text</span></code>, your request would look like the following:</p>
<p>Additional options</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
<span class="n">audio_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;/path/to/file/speech.mp3&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span>

<span class="n">transcription</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">audio</span><span class="o">.</span><span class="n">transcriptions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-transcribe&quot;</span><span class="p">,</span> 
    <span class="n">file</span><span class="o">=</span><span class="n">audio_file</span><span class="p">,</span> 
    <span class="n">response_format</span><span class="o">=</span><span class="s2">&quot;text&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">transcription</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<p>The <a class="reference external" href="https://platform.openai.com/docs/api-reference/audio">API Reference</a> includes the full list of available parameters.</p>
<p>The newer <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini-transcribe</span></code> and <code class="docutils literal notranslate"><span class="pre">gpt-4o-transcribe</span></code> models currently have a limited parameter surface: they only support <code class="docutils literal notranslate"><span class="pre">json</span></code> or <code class="docutils literal notranslate"><span class="pre">text</span></code> response formats. Other parameters, such as <code class="docutils literal notranslate"><span class="pre">timestamp_granularities</span></code>, require <code class="docutils literal notranslate"><span class="pre">verbose_json</span></code> output and are therefore only available when using <code class="docutils literal notranslate"><span class="pre">whisper-1</span></code>.</p>
</section>
<section id="translations">
<h3>Translations<a class="headerlink" href="#translations" title="Link to this heading">#</a></h3>
<p>The translations API takes as input the audio file in any of the supported languages and transcribes, if necessary, the audio into English. This differs from our /Transcriptions endpoint since the output is not in the original input language and is instead translated to English text. This endpoint supports only the <code class="docutils literal notranslate"><span class="pre">whisper-1</span></code> model.</p>
<p>Translate audio</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
<span class="n">audio_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;/path/to/file/german.mp3&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span>

<span class="n">translation</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">audio</span><span class="o">.</span><span class="n">translations</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;whisper-1&quot;</span><span class="p">,</span> 
    <span class="n">file</span><span class="o">=</span><span class="n">audio_file</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">translation</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case, the inputted audio was german and the outputted text looks like:</p>
<p>Output:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Hello, my name is Wolfgang and I come from Germany. Where are you heading today?
</pre></div>
</div>
<p>We only support translation into English at this time.</p>
</section>
</section>
<section id="supported-languages">
<h2>Supported languages<a class="headerlink" href="#supported-languages" title="Link to this heading">#</a></h2>
<p>We currently <a class="reference external" href="https://github.com/openai/whisper#available-models-and-languages">support the following languages</a> through both the <code class="docutils literal notranslate"><span class="pre">transcriptions</span></code> and <code class="docutils literal notranslate"><span class="pre">translations</span></code> endpoint:</p>
<p>Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.</p>
<p>While the underlying model was trained on 98 languages, we only list the languages that exceeded &lt;50% <a class="reference external" href="https://en.wikipedia.org/wiki/Word_error_rate">word error rate</a> (WER) which is an industry standard benchmark for speech to text model accuracy. The model will return results for languages not listed above but the quality will be low.</p>
<p>We support some ISO 639-1 and 639-3 language codes for GPT-4o based models. For language codes we don’t have, try prompting for specific languages (i.e., “Output in English”).</p>
</section>
<section id="timestamps">
<h2>Timestamps<a class="headerlink" href="#timestamps" title="Link to this heading">#</a></h2>
<p>By default, the Transcriptions API will output a transcript of the provided audio in text. The <a class="reference internal" href="#/docs/api-reference/audio/createTranscription#audio-createtranscription-timestamp_granularities"><span class="xref myst"><code class="docutils literal notranslate"><span class="pre">timestamp_granularities[]</span></code> parameter</span></a> enables a more structured and timestamped json output format, with timestamps at the segment, word level, or both. This enables word-level precision for transcripts and video edits, which allows for the removal of specific frames tied to individual words.</p>
<p>Timestamp options</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
<span class="n">audio_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;/path/to/file/speech.mp3&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span>

<span class="n">transcription</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">audio</span><span class="o">.</span><span class="n">transcriptions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
  <span class="n">file</span><span class="o">=</span><span class="n">audio_file</span><span class="p">,</span>
  <span class="n">model</span><span class="o">=</span><span class="s2">&quot;whisper-1&quot;</span><span class="p">,</span>
  <span class="n">response_format</span><span class="o">=</span><span class="s2">&quot;verbose_json&quot;</span><span class="p">,</span>
  <span class="n">timestamp_granularities</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;word&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">transcription</span><span class="o">.</span><span class="n">words</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">timestamp_granularities[]</span></code> parameter is only supported for <code class="docutils literal notranslate"><span class="pre">whisper-1</span></code>.</p>
</section>
<section id="longer-inputs">
<h2>Longer inputs<a class="headerlink" href="#longer-inputs" title="Link to this heading">#</a></h2>
<p>By default, the Transcriptions API only supports files that are less than 25 MB. If you have an audio file that is longer than that, you will need to break it up into chunks of 25 MB’s or less or used a compressed audio format. To get the best performance, we suggest that you avoid breaking the audio up mid-sentence as this may cause some context to be lost.</p>
<p>One way to handle this is to use the <a class="reference external" href="https://github.com/jiaaro/pydub">PyDub open source Python package</a> to split the audio:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pydub</span><span class="w"> </span><span class="kn">import</span> <span class="n">AudioSegment</span>

<span class="n">song</span> <span class="o">=</span> <span class="n">AudioSegment</span><span class="o">.</span><span class="n">from_mp3</span><span class="p">(</span><span class="s2">&quot;good_morning.mp3&quot;</span><span class="p">)</span>

<span class="c1"># PyDub handles time in milliseconds</span>
<span class="n">ten_minutes</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">1000</span>

<span class="n">first_10_minutes</span> <span class="o">=</span> <span class="n">song</span><span class="p">[:</span><span class="n">ten_minutes</span><span class="p">]</span>

<span class="n">first_10_minutes</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="s2">&quot;good_morning_10.mp3&quot;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;mp3&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><em>OpenAI makes no guarantees about the usability or security of 3rd party software like PyDub.</em></p>
</section>
<section id="prompting">
<h2>Prompting<a class="headerlink" href="#prompting" title="Link to this heading">#</a></h2>
<p>You can use a <a class="reference external" href="https://platform.openai.com/docs/api-reference/audio/createTranscription">prompt</a> to improve the quality of the transcripts generated by the Transcriptions API.</p>
<p>Prompting</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
<span class="n">audio_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;/path/to/file/speech.mp3&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span>

<span class="n">transcription</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">audio</span><span class="o">.</span><span class="n">transcriptions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
  <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-transcribe&quot;</span><span class="p">,</span> 
  <span class="n">file</span><span class="o">=</span><span class="n">audio_file</span><span class="p">,</span> 
  <span class="n">response_format</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
  <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;The following conversation is a lecture about the recent developments around OpenAI, GPT-4.5 and the future of AI.&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">transcription</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<p>For <code class="docutils literal notranslate"><span class="pre">gpt-4o-transcribe</span></code> and <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini-transcribe</span></code>, you can use the <code class="docutils literal notranslate"><span class="pre">prompt</span></code> parameter to improve the quality of the transcription by giving the model additional context similarly to how you would prompt other GPT-4o models.</p>
<p>Here are some examples of how prompting can help in different scenarios:</p>
<ol class="arabic simple">
<li><p>Prompts can help correct specific words or acronyms that the model misrecognizes in the audio. For example, the following prompt improves the transcription of the words DALL·E and GPT-3, which were previously written as “GDP 3” and “DALI”: “The transcript is about OpenAI which makes technology like DALL·E, GPT-3, and ChatGPT with the hope of one day building an AGI system that benefits all of humanity.”</p></li>
<li><p>To preserve the context of a file that was split into segments, prompt the model with the transcript of the preceding segment. The model uses relevant information from the previous audio, improving transcription accuracy. The <code class="docutils literal notranslate"><span class="pre">whisper-1</span></code> model only considers the final 224 tokens of the prompt and ignores anything earlier. For multilingual inputs, Whisper uses a custom tokenizer. For English-only inputs, it uses the standard GPT-2 tokenizer. Find both tokenizers in the open source <a class="reference external" href="https://github.com/openai/whisper/blob/main/whisper/tokenizer.py#L361">Whisper Python package</a>.</p></li>
<li><p>Sometimes the model skips punctuation in the transcript. To prevent this, use a simple prompt that includes punctuation: “Hello, welcome to my lecture.”</p></li>
<li><p>The model may also leave out common filler words in the audio. If you want to keep the filler words in your transcript, use a prompt that contains them: “Umm, let me think like, hmm… Okay, here’s what I’m, like, thinking.”</p></li>
<li><p>Some languages can be written in different ways, such as simplified or traditional Chinese. The model might not always use the writing style that you want for your transcript by default. You can improve this by using a prompt in your preferred writing style.</p></li>
</ol>
<p>For <code class="docutils literal notranslate"><span class="pre">whisper-1</span></code>, the model tries to match the style of the prompt, so it’s more likely to use capitalization and punctuation if the prompt does too. However, the current prompting system is more limited than our other language models and provides limited control over the generated text.</p>
<p>You can find more examples on improving your <code class="docutils literal notranslate"><span class="pre">whisper-1</span></code> transcriptions in the <a class="reference internal" href="#improving-reliability"><span class="xref myst">improving reliability</span></a> section.</p>
</section>
<section id="streaming-transcriptions">
<h2>Streaming transcriptions<a class="headerlink" href="#streaming-transcriptions" title="Link to this heading">#</a></h2>
<p>There are two ways you can stream your transcription depending on your use case and whether you are trying to transcribe an already completed audio recording or handle an ongoing stream of audio and use OpenAI for turn detection.</p>
<section id="streaming-the-transcription-of-a-completed-audio-recording">
<h3>Streaming the transcription of a completed audio recording<a class="headerlink" href="#streaming-the-transcription-of-a-completed-audio-recording" title="Link to this heading">#</a></h3>
<p>If you have an already completed audio recording, either because it’s an audio file or you are using your own turn detection (like push-to-talk), you can use our Transcription API with <code class="docutils literal notranslate"><span class="pre">stream=True</span></code> to receive a stream of <a class="reference external" href="https://platform.openai.com/docs/api-reference/audio/transcript-text-delta-event">transcript events</a> as soon as the model is done transcribing that part of the audio.</p>
<p>Stream transcriptions</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
<span class="n">audio_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;/path/to/file/speech.mp3&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span>

<span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">audio</span><span class="o">.</span><span class="n">transcriptions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
  <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini-transcribe&quot;</span><span class="p">,</span> 
  <span class="n">file</span><span class="o">=</span><span class="n">audio_file</span><span class="p">,</span> 
  <span class="n">response_format</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
  <span class="n">stream</span><span class="o">=</span><span class="kc">True</span> <span class="c1">#IMPORTANT</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
</pre></div>
</div>
<p>You will receive a stream of <code class="docutils literal notranslate"><span class="pre">transcript.text.delta</span></code> events as soon as the model is done transcribing that part of the audio, followed by a <code class="docutils literal notranslate"><span class="pre">transcript.text.done</span></code> event when the transcription is complete that includes the full transcript.</p>
<p>Additionally, you can use the <code class="docutils literal notranslate"><span class="pre">include[]</span></code> parameter to include <code class="docutils literal notranslate"><span class="pre">logprobs</span></code> in the response to get the log probabilities of the tokens in the transcription. These can be helpful to determine how confident the model is in the transcription of that particular part of the transcript.</p>
<blockquote>
<div><p>Streamed transcription is not supported in <code class="docutils literal notranslate"><span class="pre">whisper-1</span></code>.</p>
</div></blockquote>
</section>
<section id="streaming-the-transcription-of-an-ongoing-audio-recording">
<h3>Streaming the transcription of an ongoing audio recording<a class="headerlink" href="#streaming-the-transcription-of-an-ongoing-audio-recording" title="Link to this heading">#</a></h3>
<p>In the Realtime API, you can stream the transcription of an ongoing audio recording. To start a streaming session with the Realtime API, create a WebSocket connection with the following URL:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>wss://api.openai.com/v1/realtime?intent=transcription
</pre></div>
</div>
<p>Below is an example payload for setting up a transcription session:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;transcription_session.update&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;input_audio_format&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;pcm16&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;input_audio_transcription&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;gpt-4o-transcribe&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;language&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;turn_detection&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;server_vad&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;threshold&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.5</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;prefix_padding_ms&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">300</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;silence_duration_ms&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">500</span><span class="p">,</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;input_audio_noise_reduction&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;near_field&quot;</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;include&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;item.input_audio_transcription.logprobs&quot;</span>
<span class="w">  </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>To stream audio data to the API, append audio buffers:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;input_audio_buffer.append&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;audio&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Base64EncodedAudioData&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When in VAD mode, the API will respond with <code class="docutils literal notranslate"><span class="pre">input_audio_buffer.committed</span></code> every time a chunk of speech has been detected. Use <code class="docutils literal notranslate"><span class="pre">input_audio_buffer.committed.item_id</span></code> and <code class="docutils literal notranslate"><span class="pre">input_audio_buffer.committed.previous_item_id</span></code> to enforce the ordering.</p>
<p>The API responds with transcription events indicating speech start, stop, and completed transcriptions.</p>
<p>The primary resource used by the streaming ASR API is the <code class="docutils literal notranslate"><span class="pre">TranscriptionSession</span></code>:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;object&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;realtime.transcription_session&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;input_audio_format&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;pcm16&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;input_audio_transcription&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<span class="w">    </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;whisper-1&quot;</span><span class="w"> </span><span class="err">|</span><span class="w"> </span><span class="s2">&quot;gpt-4o-transcribe&quot;</span><span class="w"> </span><span class="err">|</span><span class="w"> </span><span class="s2">&quot;gpt-4o-mini-transcribe&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;language&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;string&quot;</span>
<span class="w">  </span><span class="p">}],</span>
<span class="w">  </span><span class="nt">&quot;turn_detection&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;server_vad&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;threshold&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;float&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;prefix_padding_ms&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;integer&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;silence_duration_ms&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;integer&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="err">|</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;input_audio_noise_reduction&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;near_field&quot;</span><span class="w"> </span><span class="err">|</span><span class="w"> </span><span class="s2">&quot;far_field&quot;</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;include&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;string&quot;</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Authenticate directly through the WebSocket connection using your API key or an ephemeral token obtained from:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>POST /v1/realtime/transcription_sessions
</pre></div>
</div>
<p>This endpoint returns an ephemeral token (<code class="docutils literal notranslate"><span class="pre">client_secret</span></code>) to securely authenticate WebSocket connections.</p>
</section>
</section>
<section id="improving-reliability">
<h2>Improving reliability<a class="headerlink" href="#improving-reliability" title="Link to this heading">#</a></h2>
<p>One of the most common challenges faced when using Whisper is the model often does not recognize uncommon words or acronyms. Here are some different techniques to improve the reliability of Whisper in these cases:</p>
<p>Using the prompt parameter</p>
<p>The first method involves using the optional prompt parameter to pass a dictionary of the correct spellings.</p>
<p>Because it wasn’t trained with instruction-following techniques, Whisper operates more like a base GPT model. Keep in mind that Whisper only considers the first 224 tokens of the prompt.</p>
<p>Prompt parameter</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
<span class="n">audio_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;/path/to/file/speech.mp3&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span>

<span class="n">transcription</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">audio</span><span class="o">.</span><span class="n">transcriptions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
  <span class="n">model</span><span class="o">=</span><span class="s2">&quot;whisper-1&quot;</span><span class="p">,</span> 
  <span class="n">file</span><span class="o">=</span><span class="n">audio_file</span><span class="p">,</span> 
  <span class="n">response_format</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
  <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T.&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">transcription</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<p>While it increases reliability, this technique is limited to 224 tokens, so your list of SKUs needs to be relatively small for this to be a scalable solution.</p>
<p>Post-processing with GPT-4</p>
<p>The second method involves a post-processing step using GPT-4 or GPT-3.5-Turbo.</p>
<p>We start by providing instructions for GPT-4 through the <code class="docutils literal notranslate"><span class="pre">system_prompt</span></code> variable. Similar to what we did with the prompt parameter earlier, we can define our company and product names.</p>
<p>Post-processing</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are a helpful assistant for the company ZyntriQix. Your task is to correct </span>
<span class="s2">any spelling discrepancies in the transcribed text. Make sure that the names of </span>
<span class="s2">the following products are spelled correctly: ZyntriQix, Digique Plus, </span>
<span class="s2">CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink Seven, DigiFractal </span>
<span class="s2">Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. Only add necessary </span>
<span class="s2">punctuation such as periods, commas, and capitalization, and use only the </span>
<span class="s2">context provided.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">generate_corrected_transcript</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="n">system_prompt</span><span class="p">,</span> <span class="n">audio_file</span><span class="p">):</span>
  <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
      <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4.1&quot;</span><span class="p">,</span>
      <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
      <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
          <span class="p">{</span>
              <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
              <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">system_prompt</span>
          <span class="p">},</span>
          <span class="p">{</span>
              <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
              <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">transcribe</span><span class="p">(</span><span class="n">audio_file</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
          <span class="p">}</span>
      <span class="p">]</span>
  <span class="p">)</span>
  <span class="k">return</span> <span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
<span class="n">corrected_text</span> <span class="o">=</span> <span class="n">generate_corrected_transcript</span><span class="p">(</span>
  <span class="mi">0</span><span class="p">,</span> <span class="n">system_prompt</span><span class="p">,</span> <span class="n">fake_company_filepath</span>
<span class="p">)</span>
</pre></div>
</div>
<p>If you try this on your own audio file, you’ll see that GPT-4 corrects many misspellings in the transcript. Due to its larger context window, this method might be more scalable than using Whisper’s prompt parameter. It’s also more reliable, as GPT-4 can be instructed and guided in ways that aren’t possible with Whisper due to its lack of instruction following.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="1_9_text_to_speech.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Text to speech</p>
      </div>
    </a>
    <a class="right-next"
       href="1_11_embeddings.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Vector embeddings</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quickstart">Quickstart</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transcriptions">Transcriptions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#translations">Translations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-languages">Supported languages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#timestamps">Timestamps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#longer-inputs">Longer inputs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting">Prompting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#streaming-transcriptions">Streaming transcriptions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#streaming-the-transcription-of-a-completed-audio-recording">Streaming the transcription of a completed audio recording</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#streaming-the-transcription-of-an-ongoing-audio-recording">Streaming the transcription of an ongoing audio recording</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-reliability">Improving reliability</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>