
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Vector embeddings &#8212; Agentic Prototyping</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '1_12_embeddings';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Retrieval" href="1_13_retrieval.html" />
    <link rel="prev" title="Speech to text" href="1_11_speech_to_text.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/ITRGLogoLightBlue.png" class="logo__image only-light" alt="Agentic Prototyping - Home"/>
    <script>document.write(`<img src="_static/ITRGLogoLightBlue.png" class="logo__image only-dark" alt="Agentic Prototyping - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    üóìÔ∏è Agentic AI Workshop Schedule Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Core Concepts</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_0_agent_prd.html">üìã Agent Product Requirements Document (PRD)</a></li>



<li class="toctree-l1"><a class="reference internal" href="1_0_dev_quick_start.html">Developer Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_1_prompt_engineering.html">Prompt Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_2_model_selection.html">Model Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_3_prototype_to_production.html">Prototype to Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_4_structured_outputs.html">Structured Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_5_function_calling.html">Function calling</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_6_conversational_state.html">Conversation state</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_7_streaming.html">Streaming API responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_8_file_inputs.html">File inputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_9_image_generation.html">Image generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_10_text_to_speech.html">Text to speech</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_11_speech_to_text.html">Speech to text</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Vector embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_13_retrieval.html">Retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_14_evals.html">Evaluating model performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_15_moderation.html">Moderation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Agents</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2_0_agents_overview.html">Agents Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_1_agent_design_foundations.html">Agent Design Foundations</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_2_openai_agents.html">OpenAI Agents SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_3_quick_start.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_4_building_agents.html">Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_5_running_agents.html">Running agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_6_tools.html">Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_7_mcp.html">Model context protocol (MCP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_8_handoffs.html">Handoffs</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_9_observability.html">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_10_context_management.html">Context management</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_11_guardrails.html">Guardrails</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_12_orchestration.html">Orchestrating multiple agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_13_models.html">Models</a></li>

<li class="toctree-l1"><a class="reference internal" href="2_14_visualization.html">Agent Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_15_agent_output.html">Agent Output (Results)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Best Practices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="3_0_production.html">Production best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_1_safety.html">Safety Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_2_prompt_caching.html">Prompt caching</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_3_reasoning_models.html">Reasoning Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_4_accuracy_optimization.html">Optimizing LLM Accuracy</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_5_reproducibility.html">Advanced usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_6_evaluation_design.html">Evals design best practices</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Case Studies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="4_pharma_rd.html">Use Case: AI Co-Scientist for Pharma R&amp;D</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_insurance_claims.html">Use Case: Insurance Claim Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_financial_portfolio_analysis.html">Use Case: Financial Portfolio Analysis</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/1_12_embeddings.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Vector embeddings</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-embeddings">What are embeddings?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-get-embeddings">How to get embeddings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-models">Embedding models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-cases">Use cases</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#obtaining-the-embeddings">Obtaining the embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-search-using-embeddings">Text search using embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-search-using-embeddings">Code search using embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommendations-using-embeddings">Recommendations using embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-visualization-in-2d">Data visualization in 2D</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-as-a-text-feature-encoder-for-ml-algorithms">Embedding as a text feature encoder for ML algorithms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-using-the-embedding-features">Regression using the embedding features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-using-the-embedding-features">Classification using the embedding features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-classification">Zero-shot classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#obtaining-user-and-product-embeddings-for-cold-start-recommendation">Obtaining user and product embeddings for cold-start recommendation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering">Clustering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#faq">FAQ</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-can-i-tell-how-many-tokens-a-string-has-before-i-embed-it">How can I tell how many tokens a string has before I embed it?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-can-i-retrieve-k-nearest-embedding-vectors-quickly">How can I retrieve K nearest embedding vectors quickly?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-distance-function-should-i-use">Which distance function should I use?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#can-i-share-my-embeddings-online">Can I share my embeddings online?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-v3-embedding-models-know-about-recent-events">Do V3 embedding models know about recent events?</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="vector-embeddings">
<h1>Vector embeddings<a class="headerlink" href="#vector-embeddings" title="Link to this heading">#</a></h1>
<p>Learn how to turn text into numbers, unlocking use cases like search.</p>
<p>New embedding models</p>
<p><code class="docutils literal notranslate"><span class="pre">text-embedding-3-small</span></code> and <code class="docutils literal notranslate"><span class="pre">text-embedding-3-large</span></code>, our newest and most performant embedding models, are now available. They feature lower costs, higher multilingual performance, and new parameters to control the overall size.</p>
<section id="what-are-embeddings">
<h2>What are embeddings?<a class="headerlink" href="#what-are-embeddings" title="Link to this heading">#</a></h2>
<p>OpenAI‚Äôs text embeddings measure the relatedness of text strings. Embeddings are commonly used for:</p>
<ul class="simple">
<li><p><strong>Search</strong> (where results are ranked by relevance to a query string)</p></li>
<li><p><strong>Clustering</strong> (where text strings are grouped by similarity)</p></li>
<li><p><strong>Recommendations</strong> (where items with related text strings are recommended)</p></li>
<li><p><strong>Anomaly detection</strong> (where outliers with little relatedness are identified)</p></li>
<li><p><strong>Diversity measurement</strong> (where similarity distributions are analyzed)</p></li>
<li><p><strong>Classification</strong> (where text strings are classified by their most similar label)</p></li>
</ul>
<p>An embedding is a vector (list) of floating point numbers. The <a class="reference internal" href="#which-distance-function-should-i-use"><span class="xref myst">distance</span></a> between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.</p>
<p>Visit our <a class="reference external" href="https://openai.com/api/pricing/">pricing page</a> to learn about embeddings pricing. Requests are billed based on the number of <code class="docutils literal notranslate"><span class="pre">tokens</span></code> in the <a class="reference external" href="https://platform.openai.com/docs/api-reference/embeddings/create">input</a>.</p>
</section>
<section id="how-to-get-embeddings">
<h2>How to get embeddings<a class="headerlink" href="#how-to-get-embeddings" title="Link to this heading">#</a></h2>
<p>To get an embedding, send your text string to the <a class="reference external" href="https://platform.openai.com/docs/api-reference/embeddings">embeddings API endpoint</a> along with the embedding model name (e.g., <code class="docutils literal notranslate"><span class="pre">text-embedding-3-small</span></code>):</p>
<p>Example: Getting embeddings</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="nb">input</span><span class="o">=</span><span class="s2">&quot;Your text string goes here&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
</pre></div>
</div>
<p>The response contains the embedding vector (list of floating point numbers) along with some additional metadata. You can extract the embedding vector, save it in a vector database, and use for many different use cases.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;object&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;list&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;data&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;object&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;embedding&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;index&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;embedding&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="mf">-0.006929283495992422</span><span class="p">,</span>
<span class="w">        </span><span class="mf">-0.005336422007530928</span><span class="p">,</span>
<span class="w">        </span><span class="mf">-4.547132266452536e-05</span><span class="p">,</span>
<span class="w">        </span><span class="mf">-0.024047505110502243</span>
<span class="w">      </span><span class="p">],</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;usage&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;prompt_tokens&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;total_tokens&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>By default, the length of the embedding vector is <code class="docutils literal notranslate"><span class="pre">1536</span></code> for <code class="docutils literal notranslate"><span class="pre">text-embedding-3-small</span></code> or <code class="docutils literal notranslate"><span class="pre">3072</span></code> for <code class="docutils literal notranslate"><span class="pre">text-embedding-3-large</span></code>. To reduce the embedding‚Äôs dimensions without losing its concept-representing properties, pass in the <a class="reference internal" href="#/docs/api-reference/embeddings/create#embeddings-create-dimensions"><span class="xref myst">dimensions parameter</span></a>. Find more detail on embedding dimensions in the <a class="reference internal" href="#use-cases"><span class="xref myst">embedding use case section</span></a>.</p>
</section>
<section id="embedding-models">
<h2>Embedding models<a class="headerlink" href="#embedding-models" title="Link to this heading">#</a></h2>
<p>OpenAI offers two powerful third-generation embedding model (denoted by <code class="docutils literal notranslate"><span class="pre">-3</span></code> in the model ID). Read the embedding v3 <a class="reference external" href="https://openai.com/blog/new-embedding-models-and-api-updates">announcement blog post</a> for more details.</p>
<p>Usage is priced per input token. Below is an example of pricing pages of text per US dollar (assuming ~800 tokens per page):</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>~ Pages per dollar</p></th>
<th class="head"><p>Performance on MTEB eval</p></th>
<th class="head"><p>Max input</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>text-embedding-3-small</p></td>
<td><p>62,500</p></td>
<td><p>62.3%</p></td>
<td><p>8192</p></td>
</tr>
<tr class="row-odd"><td><p>text-embedding-3-large</p></td>
<td><p>9,615</p></td>
<td><p>64.6%</p></td>
<td><p>8192</p></td>
</tr>
<tr class="row-even"><td><p>text-embedding-ada-002</p></td>
<td><p>12,500</p></td>
<td><p>61.0%</p></td>
<td><p>8192</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="use-cases">
<h2>Use cases<a class="headerlink" href="#use-cases" title="Link to this heading">#</a></h2>
<p>Here we show some representative use cases, using the <a class="reference external" href="https://www.kaggle.com/snap/amazon-fine-food-reviews">Amazon fine-food reviews dataset</a>.</p>
<section id="obtaining-the-embeddings">
<h3>Obtaining the embeddings<a class="headerlink" href="#obtaining-the-embeddings" title="Link to this heading">#</a></h3>
<p>The dataset contains a total of 568,454 food reviews left by Amazon users up to October 2012. We use a subset of the 1000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a <code class="docutils literal notranslate"><span class="pre">ProductId</span></code>, <code class="docutils literal notranslate"><span class="pre">UserId</span></code>, <code class="docutils literal notranslate"><span class="pre">Score</span></code>, review title (<code class="docutils literal notranslate"><span class="pre">Summary</span></code>) and review body (<code class="docutils literal notranslate"><span class="pre">Text</span></code>). For example:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Product Id</p></th>
<th class="head"><p>User Id</p></th>
<th class="head"><p>Score</p></th>
<th class="head"><p>Summary</p></th>
<th class="head"><p>Text</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>B001E4KFG0</p></td>
<td><p>A3SGXH7AUHU8GW</p></td>
<td><p>5</p></td>
<td><p>Good Quality Dog Food</p></td>
<td><p>I have bought several of the Vitality canned‚Ä¶</p></td>
</tr>
<tr class="row-odd"><td><p>B00813GRG4</p></td>
<td><p>A1D87F6ZCVE5NK</p></td>
<td><p>1</p></td>
<td><p>Not as Advertised</p></td>
<td><p>Product arrived labeled as Jumbo Salted Peanut‚Ä¶</p></td>
</tr>
</tbody>
</table>
</div>
<p>Below, we combine the review summary and review text into a single combined text. The model encodes this combined text and output a single vector embedding.</p>
<p><a class="reference external" href="https://cookbook.openai.com/examples/get_embeddings_from_dataset">Get_embeddings_from_dataset.ipynb</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_embedding</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">client</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="n">text</span><span class="p">],</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;ada_embedding&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">combined</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">get_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;text-embedding-3-small&#39;</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;output/embedded_1k_reviews.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>To load the data from a saved file, you can run the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;output/embedded_1k_reviews.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;ada_embedding&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">ada_embedding</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">eval</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span>
</pre></div>
</div>
<p>Reducing embedding dimensions</p>
<p>Using larger embeddings, for example storing them in a vector store for retrieval, generally costs more and consumes more compute, memory and storage than using smaller embeddings.</p>
<p>Both of our new embedding models were trained <a class="reference external" href="https://arxiv.org/abs/2205.13147">with a technique</a> that allows developers to trade-off performance and cost of using embeddings. Specifically, developers can shorten embeddings (i.e. remove some numbers from the end of the sequence) without the embedding losing its concept-representing properties by passing in the <code class="docutils literal notranslate"><span class="pre">dimensions</span></code> API parameter. For example, on the MTEB benchmark, a <code class="docutils literal notranslate"><span class="pre">text-embedding-3-large</span></code> embedding can be shortened to a size of 256 while still outperforming an unshortened <code class="docutils literal notranslate"><span class="pre">text-embedding-ada-002</span></code> embedding with a size of 1536. You can read more about how changing the dimensions impacts performance in our <a class="reference external" href="https://openai.com/blog/new-embedding-models-and-api-updates#:~:text=Native%20support%20for%20shortening%20embeddings">embeddings v3 launch blog post</a>.</p>
<p>In general, using the <code class="docutils literal notranslate"><span class="pre">dimensions</span></code> parameter when creating the embedding is the suggested approach. In certain cases, you may need to change the embedding dimension after you generate it. When you change the dimension manually, you need to be sure to normalize the dimensions of the embedding as is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">normalize_l2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">/</span> <span class="n">norm</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">norm</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span> <span class="o">/</span> <span class="n">norm</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="s2">&quot;Testing 123&quot;</span><span class="p">,</span> <span class="n">encoding_format</span><span class="o">=</span><span class="s2">&quot;float&quot;</span>
<span class="p">)</span>

<span class="n">cut_dim</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">[:</span><span class="mi">256</span><span class="p">]</span>
<span class="n">norm_dim</span> <span class="o">=</span> <span class="n">normalize_l2</span><span class="p">(</span><span class="n">cut_dim</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">norm_dim</span><span class="p">)</span>
</pre></div>
</div>
<p>Dynamically changing the dimensions enables very flexible usage. For example, when using a vector data store that only supports embeddings up to 1024 dimensions long, developers can now still use our best embedding model <code class="docutils literal notranslate"><span class="pre">text-embedding-3-large</span></code> and specify a value of 1024 for the <code class="docutils literal notranslate"><span class="pre">dimensions</span></code> API parameter, which will shorten the embedding down from 3072 dimensions, trading off some accuracy in exchange for the smaller vector size.</p>
<p>Question answering using embeddings-based search</p>
<p><a class="reference external" href="https://cookbook.openai.com/examples/question_answering_using_embeddings">Question_answering_using_embeddings.ipynb</a></p>
<p>There are many common cases where the model is not trained on data which contains key facts and information you want to make accessible when generating responses to a user query. One way of solving this, as shown below, is to put additional information into the context window of the model. This is effective in many use cases but leads to higher token costs. In this notebook, we explore the tradeoff between this approach and embeddings bases search.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;Use the below article on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found, write &quot;I don&#39;t know.&quot;</span>

<span class="s2">Article:</span>
<span class="se">\&quot;\&quot;\&quot;</span>
<span class="si">{</span><span class="n">wikipedia_article_on_curling</span><span class="si">}</span>
<span class="se">\&quot;\&quot;\&quot;</span>

<span class="s2">Question: Which athletes won the gold medal in curling at the 2022 Winter Olympics?&quot;&quot;&quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;You answer questions about the 2022 Winter Olympics.&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">query</span><span class="p">},</span>
    <span class="p">],</span>
    <span class="n">model</span><span class="o">=</span><span class="n">GPT_MODEL</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="text-search-using-embeddings">
<h3>Text search using embeddings<a class="headerlink" href="#text-search-using-embeddings" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://cookbook.openai.com/examples/semantic_text_search_using_embeddings">Semantic_text_search_using_embeddings.ipynb</a></p>
<p>To retrieve the most relevant documents we use the cosine similarity between the embedding vectors of the query and each document, and return the highest scored documents.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai.embeddings_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_embedding</span><span class="p">,</span> <span class="n">cosine_similarity</span>

<span class="k">def</span><span class="w"> </span><span class="nf">search_reviews</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">product_description</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">pprint</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embedding</span><span class="p">(</span><span class="n">product_description</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;text-embedding-3-small&#39;</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;similarities&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">ada_embedding</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">embedding</span><span class="p">))</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;similarities&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">search_reviews</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s1">&#39;delicious beans&#39;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="code-search-using-embeddings">
<h3>Code search using embeddings<a class="headerlink" href="#code-search-using-embeddings" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://cookbook.openai.com/examples/code_search_using_embeddings">Code_search.ipynb</a></p>
<p>Code search works similarly to embedding-based text search. We provide a method to extract Python functions from all the Python files in a given repository. Each function is then indexed by the <code class="docutils literal notranslate"><span class="pre">text-embedding-3-small</span></code> model.</p>
<p>To perform a code search, we embed the query in natural language using the same model. Then we calculate cosine similarity between the resulting query embedding and each of the function embeddings. The highest cosine similarity results are most relevant.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai.embeddings_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_embedding</span><span class="p">,</span> <span class="n">cosine_similarity</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;code_embedding&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;code&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">get_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;text-embedding-3-small&#39;</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">search_functions</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">code_query</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">pprint</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_lines</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embedding</span><span class="p">(</span><span class="n">code_query</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;text-embedding-3-small&#39;</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;similarities&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">code_embedding</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">embedding</span><span class="p">))</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;similarities&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">search_functions</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s1">&#39;Completions API tests&#39;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="recommendations-using-embeddings">
<h3>Recommendations using embeddings<a class="headerlink" href="#recommendations-using-embeddings" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://cookbook.openai.com/examples/recommendation_using_embeddings">Recommendation_using_embeddings.ipynb</a></p>
<p>Because shorter distances between embedding vectors represent greater similarity, embeddings can be useful for recommendation.</p>
<p>Below, we illustrate a basic recommender. It takes in a list of strings and one ‚Äòsource‚Äô string, computes their embeddings, and then returns a ranking of the strings, ranked from most similar to least similar. As a concrete example, the linked notebook below applies a version of this function to the <a class="reference external" href="http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html">AG news dataset</a> (sampled down to 2,000 news article descriptions) to return the top 5 most similar articles to any given source article.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">recommendations_from_strings</span><span class="p">(</span>
    <span class="n">strings</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">index_of_source_string</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return nearest neighbors of a given string.&quot;&quot;&quot;</span>

    <span class="c1"># get embeddings for all strings</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[</span><span class="n">embedding_from_string</span><span class="p">(</span><span class="n">string</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span> <span class="k">for</span> <span class="n">string</span> <span class="ow">in</span> <span class="n">strings</span><span class="p">]</span>

    <span class="c1"># get the embedding of the source string</span>
    <span class="n">query_embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">index_of_source_string</span><span class="p">]</span>

    <span class="c1"># get distances between the source embedding and other embeddings (function from embeddings_utils.py)</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="n">distances_from_embeddings</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">distance_metric</span><span class="o">=</span><span class="s2">&quot;cosine&quot;</span><span class="p">)</span>

    <span class="c1"># get indices of nearest neighbors (function from embeddings_utils.py)</span>
    <span class="n">indices_of_nearest_neighbors</span> <span class="o">=</span> <span class="n">indices_of_nearest_neighbors_from_distances</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">indices_of_nearest_neighbors</span>
</pre></div>
</div>
</section>
<section id="data-visualization-in-2d">
<h3>Data visualization in 2D<a class="headerlink" href="#data-visualization-in-2d" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://cookbook.openai.com/examples/visualizing_embeddings_in_2d">Visualizing_embeddings_in_2D.ipynb</a></p>
<p>The size of the embeddings varies with the complexity of the underlying model. In order to visualize this high dimensional data we use the t-SNE algorithm to transform the data into two dimensions.</p>
<p>We color the individual reviews based on the star rating which the reviewer has given:</p>
<ul class="simple">
<li><p>1-star: red</p></li>
<li><p>2-star: dark orange</p></li>
<li><p>3-star: gold</p></li>
<li><p>4-star: turquoise</p></li>
<li><p>5-star: dark green</p></li>
</ul>
<p><img alt="Amazon ratings visualized in language using t-SNE" src="https://cdn.openai.com/API/docs/images/embeddings-tsne.png" /></p>
<p>The visualization seems to have produced roughly 3 clusters, one of which has mostly negative reviews.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.manifold</span><span class="w"> </span><span class="kn">import</span> <span class="n">TSNE</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;output/embedded_1k_reviews.csv&#39;</span><span class="p">)</span>
<span class="n">matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">ada_embedding</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">eval</span><span class="p">)</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>

<span class="c1"># Create a t-SNE model and transform the data</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">vis_dims</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="s2">&quot;darkorange&quot;</span><span class="p">,</span> <span class="s2">&quot;gold&quot;</span><span class="p">,</span> <span class="s2">&quot;turquiose&quot;</span><span class="p">,</span> <span class="s2">&quot;darkgreen&quot;</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">vis_dims</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">vis_dims</span><span class="p">]</span>
<span class="n">color_indices</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">Score</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="mi">1</span>

<span class="n">colormap</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">ListedColormap</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">color_indices</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">colormap</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Amazon ratings visualized in language using t-SNE&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="embedding-as-a-text-feature-encoder-for-ml-algorithms">
<h3>Embedding as a text feature encoder for ML algorithms<a class="headerlink" href="#embedding-as-a-text-feature-encoder-for-ml-algorithms" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://cookbook.openai.com/examples/regression_using_embeddings">Regression_using_embeddings.ipynb</a></p>
<p>An embedding can be used as a general free-text feature encoder within a machine learning model. Incorporating embeddings will improve the performance of any machine learning model, if some of the relevant inputs are free text. An embedding can also be used as a categorical feature encoder within a ML model. This adds most value if the names of categorical variables are meaningful and numerous, such as job titles. Similarity embeddings generally perform better than search embeddings for this task.</p>
<p>We observed that generally the embedding representation is very rich and information dense. For example, reducing the dimensionality of the inputs using SVD or PCA, even by 10%, generally results in worse downstream performance on specific tasks.</p>
<p>This code splits the data into a training set and a testing set, which will be used by the following two use cases, namely regression and classification.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">ada_embedding</span><span class="o">.</span><span class="n">values</span><span class="p">),</span>
    <span class="n">df</span><span class="o">.</span><span class="n">Score</span><span class="p">,</span>
    <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="regression-using-the-embedding-features">
<h3>Regression using the embedding features<a class="headerlink" href="#regression-using-the-embedding-features" title="Link to this heading">#</a></h3>
<p>Embeddings present an elegant way of predicting a numerical value. In this example we predict the reviewer‚Äôs star rating, based on the text of their review. Because the semantic information contained within embeddings is high, the prediction is decent even with very few reviews.</p>
<p>We assume the score is a continuous variable between 1 and 5, and allow the algorithm to predict any floating point value. The ML algorithm minimizes the distance of the predicted value to the true score, and achieves a mean absolute error of 0.39, which means that on average the prediction is off by less than half a star.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="n">rfr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">rfr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">rfr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="classification-using-the-embedding-features">
<h3>Classification using the embedding features<a class="headerlink" href="#classification-using-the-embedding-features" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://cookbook.openai.com/examples/classification_using_embeddings">Classification_using_embeddings.ipynb</a></p>
<p>This time, instead of having the algorithm predict a value anywhere between 1 and 5, we will attempt to classify the exact number of stars for a review into 5 buckets, ranging from 1 to 5 stars.</p>
<p>After the training, the model learns to predict 1 and 5-star reviews much better than the more nuanced reviews (2-4 stars), likely due to more extreme sentiment expression.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="zero-shot-classification">
<h3>Zero-shot classification<a class="headerlink" href="#zero-shot-classification" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://cookbook.openai.com/examples/zero-shot_classification_with_embeddings">Zero-shot_classification_with_embeddings.ipynb</a></p>
<p>We can use embeddings for zero shot classification without any labeled training data. For each class, we embed the class name or a short description of the class. To classify some new text in a zero-shot manner, we compare its embedding to all class embeddings and predict the class with the highest similarity.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai.embeddings_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span><span class="p">,</span> <span class="n">get_embedding</span>

<span class="n">df</span><span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Score</span><span class="o">!=</span><span class="mi">3</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">Score</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="mi">1</span><span class="p">:</span><span class="s1">&#39;negative&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="s1">&#39;negative&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span><span class="s1">&#39;positive&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">:</span><span class="s1">&#39;positive&#39;</span><span class="p">})</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;negative&#39;</span><span class="p">,</span> <span class="s1">&#39;positive&#39;</span><span class="p">]</span>
<span class="n">label_embeddings</span> <span class="o">=</span> <span class="p">[</span><span class="n">get_embedding</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">label_score</span><span class="p">(</span><span class="n">review_embedding</span><span class="p">,</span> <span class="n">label_embeddings</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">review_embedding</span><span class="p">,</span> <span class="n">label_embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">review_embedding</span><span class="p">,</span> <span class="n">label_embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="s1">&#39;positive&#39;</span> <span class="k">if</span> <span class="n">label_score</span><span class="p">(</span><span class="s1">&#39;Sample Review&#39;</span><span class="p">,</span> <span class="n">label_embeddings</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">&#39;negative&#39;</span>
</pre></div>
</div>
</section>
<section id="obtaining-user-and-product-embeddings-for-cold-start-recommendation">
<h3>Obtaining user and product embeddings for cold-start recommendation<a class="headerlink" href="#obtaining-user-and-product-embeddings-for-cold-start-recommendation" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://cookbook.openai.com/examples/user_and_product_embeddings">User_and_product_embeddings.ipynb</a></p>
<p>We can obtain a user embedding by averaging over all of their reviews. Similarly, we can obtain a product embedding by averaging over all the reviews about that product. In order to showcase the usefulness of this approach we use a subset of 50k reviews to cover more reviews per user and per product.</p>
<p>We evaluate the usefulness of these embeddings on a separate test set, where we plot similarity of the user and product embedding as a function of the rating. Interestingly, based on this approach, even before the user receives the product we can predict better than random whether they would like the product.</p>
<p><img alt="Boxplot grouped by Score" src="https://cdn.openai.com/API/docs/images/embeddings-boxplot.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">user_embeddings</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;UserId&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">ada_embedding</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span>
<span class="n">prod_embeddings</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;ProductId&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">ada_embedding</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="clustering">
<h3>Clustering<a class="headerlink" href="#clustering" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://cookbook.openai.com/examples/clustering">Clustering.ipynb</a></p>
<p>Clustering is one way of making sense of a large volume of textual data. Embeddings are useful for this task, as they provide semantically meaningful vector representations of each text. Thus, in an unsupervised way, clustering will uncover hidden groupings in our dataset.</p>
<p>In this example, we discover four distinct clusters: one focusing on dog food, one on negative reviews, and two on positive reviews.</p>
<p><img alt="Clusters identified visualized in language 2d using t-SNE" src="https://cdn.openai.com/API/docs/images/embeddings-cluster.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">ada_embedding</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="n">n_clusters</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span>
</pre></div>
</div>
</section>
</section>
<section id="faq">
<h2>FAQ<a class="headerlink" href="#faq" title="Link to this heading">#</a></h2>
<section id="how-can-i-tell-how-many-tokens-a-string-has-before-i-embed-it">
<h3>How can I tell how many tokens a string has before I embed it?<a class="headerlink" href="#how-can-i-tell-how-many-tokens-a-string-has-before-i-embed-it" title="Link to this heading">#</a></h3>
<p>In Python, you can split a string into tokens with OpenAI‚Äôs tokenizer <a class="reference external" href="https://github.com/openai/tiktoken"><code class="docutils literal notranslate"><span class="pre">tiktoken</span></code></a>.</p>
<p>Example code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>

<span class="k">def</span><span class="w"> </span><span class="nf">num_tokens_from_string</span><span class="p">(</span><span class="n">string</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">encoding_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the number of tokens in a text string.&quot;&quot;&quot;</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="n">encoding_name</span><span class="p">)</span>
    <span class="n">num_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">string</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">num_tokens</span>

<span class="n">num_tokens_from_string</span><span class="p">(</span><span class="s2">&quot;tiktoken is great!&quot;</span><span class="p">,</span> <span class="s2">&quot;cl100k_base&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>For third-generation embedding models like <code class="docutils literal notranslate"><span class="pre">text-embedding-3-small</span></code>, use the <code class="docutils literal notranslate"><span class="pre">cl100k_base</span></code> encoding.</p>
<p>More details and example code are in the OpenAI Cookbook guide <a class="reference external" href="https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken">how to count tokens with tiktoken</a>.</p>
</section>
<section id="how-can-i-retrieve-k-nearest-embedding-vectors-quickly">
<h3>How can I retrieve K nearest embedding vectors quickly?<a class="headerlink" href="#how-can-i-retrieve-k-nearest-embedding-vectors-quickly" title="Link to this heading">#</a></h3>
<p>For searching over many vectors quickly, we recommend using a vector database. You can find examples of working with vector databases and the OpenAI API <a class="reference external" href="https://cookbook.openai.com/examples/vector_databases/readme">in our Cookbook</a> on GitHub.</p>
</section>
<section id="which-distance-function-should-i-use">
<h3>Which distance function should I use?<a class="headerlink" href="#which-distance-function-should-i-use" title="Link to this heading">#</a></h3>
<p>We recommend <a class="reference external" href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>. The choice of distance function typically doesn‚Äôt matter much.</p>
<p>OpenAI embeddings are normalized to length 1, which means that:</p>
<ul class="simple">
<li><p>Cosine similarity can be computed slightly faster using just a dot product</p></li>
<li><p>Cosine similarity and Euclidean distance will result in the identical rankings</p></li>
</ul>
</section>
<section id="can-i-share-my-embeddings-online">
<h3>Can I share my embeddings online?<a class="headerlink" href="#can-i-share-my-embeddings-online" title="Link to this heading">#</a></h3>
<p>Yes, customers own their input and output from our models, including in the case of embeddings. You are responsible for ensuring that the content you input to our API does not violate any applicable law or our <a class="reference external" href="https://openai.com/policies/terms-of-use">Terms of Use</a>.</p>
</section>
<section id="do-v3-embedding-models-know-about-recent-events">
<h3>Do V3 embedding models know about recent events?<a class="headerlink" href="#do-v3-embedding-models-know-about-recent-events" title="Link to this heading">#</a></h3>
<p>No, the <code class="docutils literal notranslate"><span class="pre">text-embedding-3-large</span></code> and <code class="docutils literal notranslate"><span class="pre">text-embedding-3-small</span></code> models lack knowledge of events that occurred after September 2021. This is generally not as much of a limitation as it would be for text generation models but in certain edge cases it can reduce performance.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="1_11_speech_to_text.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Speech to text</p>
      </div>
    </a>
    <a class="right-next"
       href="1_13_retrieval.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Retrieval</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-embeddings">What are embeddings?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-get-embeddings">How to get embeddings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-models">Embedding models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-cases">Use cases</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#obtaining-the-embeddings">Obtaining the embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-search-using-embeddings">Text search using embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-search-using-embeddings">Code search using embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recommendations-using-embeddings">Recommendations using embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-visualization-in-2d">Data visualization in 2D</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-as-a-text-feature-encoder-for-ml-algorithms">Embedding as a text feature encoder for ML algorithms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-using-the-embedding-features">Regression using the embedding features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-using-the-embedding-features">Classification using the embedding features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-classification">Zero-shot classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#obtaining-user-and-product-embeddings-for-cold-start-recommendation">Obtaining user and product embeddings for cold-start recommendation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering">Clustering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#faq">FAQ</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-can-i-tell-how-many-tokens-a-string-has-before-i-embed-it">How can I tell how many tokens a string has before I embed it?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-can-i-retrieve-k-nearest-embedding-vectors-quickly">How can I retrieve K nearest embedding vectors quickly?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#which-distance-function-should-i-use">Which distance function should I use?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#can-i-share-my-embeddings-online">Can I share my embeddings online?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-v3-embedding-models-know-about-recent-events">Do V3 embedding models know about recent events?</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>