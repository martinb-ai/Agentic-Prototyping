{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developer Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI API provides a simple interface to state-of-the-art AI models for text generation, natural language processing, computer vision, and more. This example generates text output from a prompt, as you might using ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize Openai Client library and Setup API keys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the client with API key from environment\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under a silver moon, a gentle unicorn named Luna tiptoed through a sparkling forest, leaving trails of twinkling stardust and sweet dreams for all the sleeping creatures.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Image Inputs\n",
    "\n",
    "You can provide image inputs to the model as well. Scan receipts, analyze screenshots, or find objects in the real world with computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": \"What two dogs are in this image?\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Comparison_of_a_wolf_and_a_pug.png/1920px-Comparison_of_a_wolf_and_a_pug.png\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a Base64 encoded Image (local images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is a diagram depicting a task delegation workflow, likely illustrating how a multi-agent system works in a language translation context.\n",
      "\n",
      "Here's what's shown:\n",
      "\n",
      "- On the left, a user input is described:  \n",
      "  `\"Translate 'hello' to Spanish, French and Italian for me!\"`\n",
      "- The request is sent to a central box labeled **Manager**.\n",
      "- The **Manager** breaks the task into three separate **Task** components (in dashed boxes).\n",
      "- Each **Task** is directed to a specific agent:\n",
      "  - **Spanish agent**\n",
      "  - **French agent**\n",
      "  - **Italian agent**\n",
      "- The agents are responsible for handling the translation for their respective languages.\n",
      "- There's also an indication that multiple other requests (\"...\") can be managed by this system.\n",
      "\n",
      "In summary, the diagram illustrates a system where a manager coordinates multiple agents to perform subtasks—in this case, language translation—by distributing and collecting tasks efficiently.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"images/multi_agent_orchestration.png\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"what's in this image?\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify image input detail level\n",
    "\n",
    "The `detail` parameter tells the model what level of detail to use when processing and understanding the image (`low`, `high`, or `auto` to let the model decide). If you skip the parameter, the model will use auto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"type\": \"input_image\",\n",
    "    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "    \"detail\": \"high\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save tokens and speed up responses by using `\"detail\": \"low\"`. This lets the model process the image with a budget of 85 tokens. The model receives a low-resolution 512px x 512px version of the image. This is fine if your use case doesn't require the model to see with high-resolution detail (for example, if you're asking about the dominant shape or color in the image).\n",
    "\n",
    "On the other hand, you can use `\"detail\": \"high\"` if you want the model to have a better understanding of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Input Requirements\n",
    "\n",
    "Input images must meet the following requirements to be used in the API.\n",
    "\n",
    "##### Supported File Types\n",
    "- PNG (.png)\n",
    "- JPEG (.jpeg and .jpg)\n",
    "- WEBP (.webp)\n",
    "- Non-animated GIF (.gif)\n",
    "\n",
    "##### Size Limits\n",
    "- Up to 50 MB total payload size per request\n",
    "- Up to 500 individual image inputs per request\n",
    "\n",
    "##### Other Requirements\n",
    "- No watermarks or logos\n",
    "- No NSFW content\n",
    "- Clear enough for a human to understand\n",
    "\n",
    "\n",
    "##### Limitations\n",
    "While models with vision capabilities are powerful and can be used in many situations, it's important to understand the limitations of these models. Here are some known limitations:\n",
    "\n",
    "- Medical images: The model is not suitable for interpreting specialized medical images like CT scans and shouldn't be used for medical advice.\n",
    "- Non-English: The model may not perform optimally when handling images with text of non-Latin alphabets, such as Japanese or Korean.\n",
    "- Small text: Enlarge text within the image to improve readability, but avoid cropping important details.\n",
    "- Rotation: The model may misinterpret rotated or upside-down text and images.\n",
    "- Visual elements: The model may struggle to understand graphs or text where colors or styles—like solid, dashed, or dotted lines—vary.\n",
    "- Spatial reasoning: The model struggles with tasks requiring precise spatial localization, such as identifying chess positions.\n",
    "- Accuracy: The model may generate incorrect descriptions or captions in certain scenarios.\n",
    "- Image shape: The model struggles with panoramic and fisheye images.\n",
    "- Metadata and resizing: The model doesn't process original file names or metadata, and images are resized before analysis, affecting their original dimensions.\n",
    "- Counting: The model may give approximate counts for objects in images.\n",
    "- CAPTCHAS: For safety reasons, our system blocks the submission of CAPTCHAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend the model with tools\n",
    "\n",
    "Give the model access to new data and capabilities using tools. You can either call your own custom code, or use one of OpenAI's powerful built-in tools. This example uses web search to give the model access to the latest information on the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of June 23, 2025, one notable positive news story is the successful reintroduction of helmeted honeyeaters to Cardinia in Victoria, Australia. These critically endangered birds have returned to the area for the first time since the Ash Wednesday bushfires in 1983, marking a significant milestone in conservation efforts. ([globalgoodnews.com](https://globalgoodnews.com/?utm_source=openai))\n",
      "\n",
      "Additionally, a 14-year-old from Dallas, Siddharth Nandyala, has developed an AI-powered app capable of detecting heart disease in just seven seconds using only a smartphone's microphone. This innovation has the potential to revolutionize early detection and treatment of heart conditions. ([globalgoodnews.com](https://globalgoodnews.com/?utm_source=openai))\n",
      "\n",
      "Furthermore, the Yurok Tribe in the United States is celebrating the return of ancestral homelands, following historic dam removals. This restoration supports the tribe's cultural and environmental initiatives. ([goodnewsnetwork.org](https://www.goodnewsnetwork.org/?utm_source=openai))\n",
      "\n",
      "These stories highlight significant advancements in environmental conservation, technological innovation, and cultural restoration. \n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=\"What was a positive news story from today?\"\n",
    ")\n",
    "\n",
    "#print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliver blazing fast AI experiences\n",
    "\n",
    "Using either the new Realtime API or server-sent streaming events, you can build high performance, low-latency experiences for your users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say 'double bubble bath'.\",\n",
    "        },\n",
    "    ],\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build agents\n",
    "\n",
    "Use the OpenAI platform to build agents capable of taking action—like controlling computers—on behalf of your users. Use the Agents SDK for Python or TypeScript to create orchestration logic on the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, estoy bien, gracias. ¿Y tú, cómo estás?\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from agents import Agent, Runner\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"Spanish agent\",\n",
    "    instructions=\"You only speak Spanish.\",\n",
    ")\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\",\n",
    "    instructions=\"You only speak English\",\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
    "    handoffs=[spanish_agent, english_agent],\n",
    ")\n",
    "\n",
    "# Define your async function\n",
    "async def main():\n",
    "    result = await Runner.run(triage_agent, input=\"Hola, ¿cómo estás?\")\n",
    "    return result.final_output\n",
    "\n",
    "# In Jupyter, use await directly\n",
    "result = await main()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
