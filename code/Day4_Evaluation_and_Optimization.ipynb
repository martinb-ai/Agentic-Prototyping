{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4: Evaluation & Optimization - Hands-On Notebook\n",
    "\n",
    "Welcome to the Day 4 hands-on notebook! Today is all about measuring and improving your agent's performance through systematic evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1: Creating and Running Your First Eval\n",
    "\n",
    "We will create an evaluation to test an agent's ability to correctly classify support tickets. This requires using the `openai` library, but not the `openai-agents` SDK, as evals are a platform feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# 1. Define the Evaluation\n",
    "eval_name = "Support Ticket Classifier Eval v1"\n",
    "eval_obj = client.evals.create(\n",
    "    name=eval_name,\n",
    "    data_source_config={\n",
    "        "type": "custom",\n",
    "        "item_schema": {\n",
    "            "type": "object",\n",
    "            "properties": {\n",
    "                "ticket_text": {"type": "string"},\n",
    "                "correct_label": {"type": "string"},\n",
    "            },\n",
    "            "required": ["ticket_text", "correct_label"],\n",
    "        },\n",
    "        "include_sample_schema": True,\n",
    "    },\n",
    "    testing_criteria=[\n",
    "        {\n",
    "            "type": "string_check",\n",
    "            "name": "Exact Match",\n",
    "            "input": "{{ sample.output_text }}",\n",
    "            "operation": "eq",\n",
    "            "reference": "{{ item.correct_label }}",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "eval_id = eval_obj.id\n",
    "print(f"Eval '{eval_name}' created with ID: {eval_id}")\n",
    "\n",
    "# 2. Create and Upload the Dataset\n",
    "dataset = [\n",
    "    {"item": {"ticket_text": "My laptop screen is cracked.", "correct_label": "Hardware"}},\n",
    "    {"item": {"ticket_text": "I can't install the new software update.", "correct_label": "Software"}},\n",
    "    {"item": {"ticket_text": "The power cord is frayed.", "correct_label": "Hardware"}},\n",
    "    {"item": {"ticket_text": "My account is locked.", "correct_label": "Software"}},\n",
    "    {"item": {"ticket_text": "Where is your office located?", "correct_label": "Other"}},\n",
    "]\n",
    "\n",
    "dataset_filename = "ticket_eval_dataset.jsonl"\n",
    "with open(dataset_filename, 'w') as f:\n",
    "    for entry in dataset:\n",
    "        f.write(json.dumps(entry) + '\n')\n",
    "\n",
    "file_obj = client.files.create(file=open(dataset_filename, "rb"), purpose="evals")\n",
    "file_id = file_obj.id\n",
    "print(f"Dataset uploaded with File ID: {file_id}")\n",
    "\n",
    "# 3. Define the Prompt to Test and Run the Eval\n",
    "prompt_to_test = [\n",
    "    {"role": "system", "content": "You are an IT support ticket classifier. Categorize the ticket into Hardware, Software, or Other. Respond with only one word."},\n",
    "    {"role": "user", "content": "{{ item.ticket_text }}"},\n",
    "]\n",
    "\n",
    "run = client.evals.runs.create(\n",
    "    eval_id,\n",
    "    name="Ticket Classifier Test Run v1",\n",
    "    data_source={\n",
    "        "type": "completions",\n",
    "        "model": "gpt-4.1",\n",
    "        "input_messages": {\n",
    "            "type": "template",\n",
    "            "template": prompt_to_test,\n",
    "        },\n",
    "        "source": {"type": "file_id", "id": file_id},\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f"Eval run created! View progress at: {run.report_url}")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2: The Optimization Challenge\n",
    "\n",
    "After the eval run completes, analyze the results in the dashboard. Let's assume one of the tests failed because the model responded with "It seems like a Hardware issue." instead of just "Hardware". This is a **behavior problem** (consistency/formatting), not a context problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis:** By making our instructions more explicit and adding an example, we can improve the model's adherence to the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define an improved prompt\n",
    "improved_prompt = [\n",
    "    {\n",
    "        "role": "system", \n",
    "        "content": "\"\"You are an IT support ticket classifier. Your task is to categorize the user's ticket into one of three categories: Hardware, Software, or Other.\n",
    "IMPORTANT: You must respond with ONLY a single word: Hardware, Software, or Other.\n",
    "\n",
    "Example:\n",
    "User: My mouse is not working.\n",
    "Assistant: Hardware\"\""\n",
    "    },\n",
    "    {"role": "user", "content": "{{ item.ticket_text }}"},\n",
    "]\n",
    "\n",
    "# Now, create a new eval run with the improved prompt\n",
    "run_v2 = client.evals.runs.create(\n",
    "    eval_id,\n",
    "    name="Ticket Classifier Test Run v2 (Improved Prompt)",\n",
    "    data_source={\n",
    "        "type": "completions",\n",
    "        "model": "gpt-4.1",\n",
    "        "input_messages": {\n",
    "            "type": "template",\n",
    "            "template": improved_prompt,\n",
    "        },\n",
    "        "source": {"type": "file_id", "id": file_id},\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f"Eval run v2 created! View progress at: {run_v2.report_url}")\n",
    "\n",
    "# After this run completes, compare the results of v1 and v2 in the dashboard.\n",
    "# Did the pass rate improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3: Designing an Eval for a Multi-Agent System\n",
    "\n",
    "This is a design exercise. Think about the multi-agent system you designed on Day 2 (Triage -> Specialist). How would you evaluate its performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluating the Triage Agent\n",
    "\n",
    "**Objective:** The triage agent must hand off to the correct specialist.\n",
    "\n",
    "**Dataset Schema (`item_schema`):**\n",
    "```json\n",
    "{ \n",
    "  "type": "object", \n",
    "  "properties": { \n",
    "    "user_query": {"type": "string"}, \n",
    "    "correct_handoff_agent_name": {"type": "string"} \n",
    "  }"\n",
    "}\n",
    "```\n",
    "\n",
    "**Testing Criteria:**\n",
    "This is tricky because the output we want to check isn't a simple string; it's the *name of the agent that ran last*. We can't use a simple `string_check`. We would need a more advanced eval, likely an **LLM-as-a-judge** eval.\n",
    "\n",
    "The judge's prompt would look something like this:\n",
    "\n",
    "```\n",
    "You are an evaluator. You will be given a user query and the name of the agent that was handed off to. Determine if the handoff was correct.\n",
    "\n",
    "User Query: {{ item.user_query }}\n",
    "Correct Handoff: {{ item.correct_handoff_agent_name }}\n",
    "Actual Handoff: {{ sample.last_agent.name }}\n",
    "\n",
    "Does the Actual Handoff match the Correct Handoff? Respond with only "Yes" or "No".\n",
    "```\n",
    "\n",
    "We would then use a `string_check` to see if the judge's output is "Yes"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluating a Specialist Agent's Tool Use\n",
    "\n",
    "**Objective:** The Shipping Specialist must call the `get_tracking_status` tool with the correct tracking number.\n",
    "\n",
    "**Dataset Schema (`item_schema`):**\n",
    "```json\n",
    "{ \n",
    "  "type": "object", \n",
    "  "properties": { \n",
    "    "user_query": {"type": "string"}, \n",
    "    "expected_tracking_number": {"type": "string"} \n",
    "  }"\n",
    "}\n",
    "```\n",
    "\n",
    "**Testing Criteria:**\n",
    "Again, this requires an LLM-as-a-judge. We need to inspect the `tool_calls` in the agent's run history.\n",
    "\n",
    "The judge's prompt:\n",
    "\n",
    "```\n",
    "You are an evaluator. Check if the `get_tracking_status` tool was called with the correct tracking number.\n",
    "\n",
    "User Query: {{ item.user_query }}\n",
    "Expected Tracking Number: {{ item.expected_tracking_number }}\n",
    "Actual Tool Calls: {{ sample.tool_calls }} \n",
    "\n",
    "Was the `get_tracking_status` tool called with the expected tracking number? Respond with only "Yes" or "No".\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
